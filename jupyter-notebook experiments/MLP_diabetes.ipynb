{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/sujeongim/Documents/AI+CS/Research/BloodGlucose-Prediction/jupyter-notebook experiments/MLP_diabetes.ipynb Cell 1'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/sujeongim/Documents/AI%2BCS/Research/BloodGlucose-Prediction/jupyter-notebook%20experiments/MLP_diabetes.ipynb#ch0000000?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sujeongim/Documents/AI%2BCS/Research/BloodGlucose-Prediction/jupyter-notebook%20experiments/MLP_diabetes.ipynb#ch0000000?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sujeongim/Documents/AI%2BCS/Research/BloodGlucose-Prediction/jupyter-notebook%20experiments/MLP_diabetes.ipynb#ch0000000?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from statsmodels.tools.eval_measures import rmse\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, TimeDistributed\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, Callback\n",
    "import warnings\n",
    "import io\n",
    "import math\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(a, n=10): \n",
    "    ret = np.cumsum(a, dtype=float)\n",
    "    ret[n:] = ret[n:] - ret[:-n]\n",
    "    ret/=n\n",
    "    # Masking\n",
    "    ret[:n-1]=ret[n-1]\n",
    "    ret=ret.reshape(-1,1)\n",
    "    return ret\n",
    "\n",
    "def read_train_patients(input_len = 8, output_len = 6):\n",
    "    x_train=[]\n",
    "    y_train=[]\n",
    "    #y_train_f=[]\n",
    "    for i in range(1, 15):\n",
    "        #y_noised=[]\n",
    "        \n",
    "        df=pd.read_csv(\"Virtual_patients_20\\V_CGMS_{}.csv\".format(i),header=None)\n",
    "        dataset=df.values\n",
    "        dataset = moving_average(dataset)\n",
    "\n",
    "        scaler = MinMaxScaler(feature_range=(0, 1)) \n",
    "        scaled_data = scaler.fit_transform(dataset)\n",
    "        train_data = scaled_data\n",
    "        #train_data = dataset\n",
    "        \n",
    "        for i in range(input_len,len(train_data)-output_len):\n",
    "            x_train.append(train_data[i-input_len:i,0])\n",
    "            y_train.append(train_data[i+output_len-1,0])\n",
    "        \n",
    "    x_train, y_train = np.array(x_train), np.array(y_train)\n",
    "    #reshape \n",
    "    #x_train = np.reshape(x_train, (x_train.shape[0],x_train.shape[1],1))\n",
    "    \n",
    "    return scaler, x_train, y_train\n",
    "    #return x_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(x_train, y_train, batch_size=1, epochs=1):\n",
    "    \n",
    "    X_val = x_train[11000:]\n",
    "    Y_val = y_train[11000:]\n",
    "    X_train = x_train[:11000]\n",
    "    Y_train = y_train[:11000]\n",
    "\n",
    "    \n",
    "    # MLP model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(32,input_dim=8,activation=\"relu\"))\n",
    "    #model.add(Dropout(0.3))\n",
    "    model.add(Dense(32,activation=\"relu\"))\n",
    "    #model.add(Dropout(0.3))\n",
    "    model.add(Dense(32, activation=\"relu\"))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(1))\n",
    "    print(model.summary())\n",
    "\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "    #over fitting 방지\n",
    "    early_stopping = EarlyStopping(monitor='loss', patience=50, mode='auto')\n",
    "    \n",
    "    #fit model\n",
    "    #hist = model.fit(X_train, Y_train, batch_size=batch_size, epochs=epochs, callbacks=[early_stopping],validation_data=(X_val, Y_val))\n",
    "    hist = model.fit(X_train, Y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_val, Y_val))\n",
    "    \n",
    "\n",
    "    # 학습과정 살펴보기\n",
    "    \"\"\"plt.plot(hist.history['loss'])\n",
    "    plt.plot(hist.history['val_loss'])\n",
    "    plt.ylim(0.0, 0.15)\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'val'], loc='upper left')\n",
    "    plt.show()\"\"\"\n",
    "    return model, hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler, x_train, y_train = read_train_patients()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 32)                288       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 2,433\n",
      "Trainable params: 2,433\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/500\n",
      "344/344 [==============================] - 2s 3ms/step - loss: 0.0239 - val_loss: 0.0092\n",
      "Epoch 2/500\n",
      "344/344 [==============================] - 0s 1ms/step - loss: 0.0139 - val_loss: 0.0100\n",
      "Epoch 3/500\n",
      "344/344 [==============================] - 0s 861us/step - loss: 0.0124 - val_loss: 0.0087\n",
      "Epoch 4/500\n",
      "344/344 [==============================] - 0s 849us/step - loss: 0.0120 - val_loss: 0.0085\n",
      "Epoch 5/500\n",
      "344/344 [==============================] - 0s 846us/step - loss: 0.0118 - val_loss: 0.0105\n",
      "Epoch 6/500\n",
      "344/344 [==============================] - 0s 834us/step - loss: 0.0115 - val_loss: 0.0084\n",
      "Epoch 7/500\n",
      "344/344 [==============================] - 0s 878us/step - loss: 0.0122 - val_loss: 0.0088\n",
      "Epoch 8/500\n",
      "344/344 [==============================] - 0s 925us/step - loss: 0.0111 - val_loss: 0.0087\n",
      "Epoch 9/500\n",
      "344/344 [==============================] - 0s 837us/step - loss: 0.0109 - val_loss: 0.0112\n",
      "Epoch 10/500\n",
      "344/344 [==============================] - 0s 852us/step - loss: 0.0107 - val_loss: 0.0084\n",
      "Epoch 11/500\n",
      "344/344 [==============================] - 0s 852us/step - loss: 0.0114 - val_loss: 0.0099\n",
      "Epoch 12/500\n",
      "344/344 [==============================] - 0s 858us/step - loss: 0.0111 - val_loss: 0.0081\n",
      "Epoch 13/500\n",
      "344/344 [==============================] - 0s 837us/step - loss: 0.0106 - val_loss: 0.0084\n",
      "Epoch 14/500\n",
      "344/344 [==============================] - 0s 904us/step - loss: 0.0105 - val_loss: 0.0081\n",
      "Epoch 15/500\n",
      "344/344 [==============================] - 0s 907us/step - loss: 0.0107 - val_loss: 0.0095\n",
      "Epoch 16/500\n",
      "344/344 [==============================] - 0s 852us/step - loss: 0.0108 - val_loss: 0.0082\n",
      "Epoch 17/500\n",
      "344/344 [==============================] - 0s 910us/step - loss: 0.0105 - val_loss: 0.0080\n",
      "Epoch 18/500\n",
      "344/344 [==============================] - 0s 936us/step - loss: 0.0107 - val_loss: 0.0081\n",
      "Epoch 19/500\n",
      "344/344 [==============================] - 0s 901us/step - loss: 0.0110 - val_loss: 0.0080\n",
      "Epoch 20/500\n",
      "344/344 [==============================] - 0s 951us/step - loss: 0.0105 - val_loss: 0.0094\n",
      "Epoch 21/500\n",
      "344/344 [==============================] - 0s 878us/step - loss: 0.0108 - val_loss: 0.0084\n",
      "Epoch 22/500\n",
      "344/344 [==============================] - 0s 907us/step - loss: 0.0108 - val_loss: 0.0084\n",
      "Epoch 23/500\n",
      "344/344 [==============================] - 0s 887us/step - loss: 0.0104 - val_loss: 0.0086\n",
      "Epoch 24/500\n",
      "344/344 [==============================] - 0s 820us/step - loss: 0.0102 - val_loss: 0.0078\n",
      "Epoch 25/500\n",
      "344/344 [==============================] - 0s 922us/step - loss: 0.0107 - val_loss: 0.0083\n",
      "Epoch 26/500\n",
      "344/344 [==============================] - 0s 861us/step - loss: 0.0105 - val_loss: 0.0078\n",
      "Epoch 27/500\n",
      "344/344 [==============================] - 0s 913us/step - loss: 0.0104 - val_loss: 0.0084\n",
      "Epoch 28/500\n",
      "344/344 [==============================] - 0s 820us/step - loss: 0.0104 - val_loss: 0.0079\n",
      "Epoch 29/500\n",
      "344/344 [==============================] - 0s 852us/step - loss: 0.0103 - val_loss: 0.0084\n",
      "Epoch 30/500\n",
      "344/344 [==============================] - 0s 843us/step - loss: 0.0105 - val_loss: 0.0078\n",
      "Epoch 31/500\n",
      "344/344 [==============================] - 0s 814us/step - loss: 0.0097 - val_loss: 0.0079\n",
      "Epoch 32/500\n",
      "344/344 [==============================] - 0s 954us/step - loss: 0.0106 - val_loss: 0.0083\n",
      "Epoch 33/500\n",
      "344/344 [==============================] - 0s 974us/step - loss: 0.0103 - val_loss: 0.0080\n",
      "Epoch 34/500\n",
      "344/344 [==============================] - 0s 928us/step - loss: 0.0101 - val_loss: 0.0082\n",
      "Epoch 35/500\n",
      "344/344 [==============================] - 0s 858us/step - loss: 0.0105 - val_loss: 0.0083\n",
      "Epoch 36/500\n",
      "344/344 [==============================] - 0s 837us/step - loss: 0.0100 - val_loss: 0.0085\n",
      "Epoch 37/500\n",
      "344/344 [==============================] - 0s 861us/step - loss: 0.0104 - val_loss: 0.0082\n",
      "Epoch 38/500\n",
      "344/344 [==============================] - 0s 922us/step - loss: 0.0106 - val_loss: 0.0079\n",
      "Epoch 39/500\n",
      "344/344 [==============================] - 0s 881us/step - loss: 0.0106 - val_loss: 0.0088\n",
      "Epoch 40/500\n",
      "344/344 [==============================] - 0s 797us/step - loss: 0.0105 - val_loss: 0.0083\n",
      "Epoch 41/500\n",
      "344/344 [==============================] - 0s 811us/step - loss: 0.0102 - val_loss: 0.0078\n",
      "Epoch 42/500\n",
      "344/344 [==============================] - 0s 901us/step - loss: 0.0104 - val_loss: 0.0078\n",
      "Epoch 43/500\n",
      "344/344 [==============================] - 0s 834us/step - loss: 0.0100 - val_loss: 0.0080\n",
      "Epoch 44/500\n",
      "344/344 [==============================] - 0s 904us/step - loss: 0.0099 - val_loss: 0.0078\n",
      "Epoch 45/500\n",
      "344/344 [==============================] - 0s 1ms/step - loss: 0.0099 - val_loss: 0.0084\n",
      "Epoch 46/500\n",
      "293/344 [========================>.....] - ETA: 0s - loss: 0.0097"
     ]
    }
   ],
   "source": [
    "model, hist = train_model(x_train, y_train, batch_size=32, epochs=500) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_loss(hist):\n",
    "    plt.figure(figsize=(24,16))\n",
    "    plt.title('Loss after smoothing',fontsize=50)\n",
    "    plt.plot(hist.history['loss'], linewidth=5)\n",
    "    plt.plot(hist.history['val_loss'], linewidth=5)\n",
    "    plt.ylim(0, 0.0600)\n",
    "    ax=plt.axes()\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(0.005))\n",
    "    plt.xticks(fontsize=33)\n",
    "    plt.yticks(fontsize=33)\n",
    "    plt.ylabel('loss',fontsize=44)\n",
    "    plt.xlabel('epoch',fontsize=44)\n",
    "    plt.legend(['train', 'val'], loc='upper right', fontsize=44)\n",
    "    plt.show()\n",
    "\n",
    "plot_loss(hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#이건 바꾸지 말기 - 비교해야 되니깐\n",
    "def show_plots(i, continuous_ytest, continuous_predictions):\n",
    "    plt.figure(figsize=(24,16))\n",
    "    plt.title('Blood Glucose Prediction Model Result_patient_{}'.format(i),fontsize=40)\n",
    "    plt.plot(continuous_ytest, color = 'b', linewidth=3)\n",
    "    plt.plot(continuous_predictions, color = 'r', ls=\"--\", linewidth=3)\n",
    "    plt.xticks(fontsize=28)\n",
    "    plt.yticks(fontsize=28)\n",
    "    plt.ylim(40, 260)\n",
    "    ax=plt.axes()\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(20))\n",
    "    plt.xlabel('Timestamp',fontsize=28)\n",
    "    plt.ylabel('BG(CGM) (mg/dL)',fontsize=28)\n",
    "    plt.legend(['Real','Predictions'], loc='upper right',fontsize=32)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, i, input_len=8, output_len=6):\n",
    "    df = pd.read_csv(\"Virtual_patients_20\\V_CGMS_{}.csv\".format(i),header=None)\n",
    "    dataset=df.values\n",
    "    \n",
    "    dataset = moving_average(dataset)\n",
    "    \n",
    "    # Scalling data from 0 - 1 to input in the neural network\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1)) \n",
    "    scaled_data = scaler.fit_transform(dataset)\n",
    "    \n",
    "    x_test =[]\n",
    "    y_test =[]\n",
    "    \n",
    "    continuous_ytest=[]\n",
    "    i= input_len\n",
    "    \n",
    "    for i in range(input_len,len(dataset)-output_len):\n",
    "        x_test.append(scaled_data[i-input_len:i,0])\n",
    "        y_test.append(dataset[i+output_len-1,0])\n",
    "        continuous_ytest.append(dataset[i+output_len-1,0])\n",
    "\n",
    "    x_test = np.array(x_test)\n",
    "    y_test = np.array(y_test)\n",
    "    #y_test = np.reshape(y_test,(-1,1))\n",
    "    #print(\"x_test: \", x_test)\n",
    "    #x_test= np.reshape(x_test, (x_test.shape[0],x_test.shape[1],1))\n",
    "    \n",
    "    predictions = model.predict(x_test)\n",
    "    predictions = np.reshape(predictions,(1,-1))\n",
    "    predictions = scaler.inverse_transform(predictions)\n",
    "    print(predictions)\n",
    "    print(y_test)\n",
    "    #print(\"predictions: \", predictions)\n",
    "    \n",
    "    continuous_predictions = predictions[0]\n",
    "    for i in range(1, len(predictions)):\n",
    "        continuous_predictions = np.concatenate([continuous_predictions, predictions[i]])\n",
    "    \n",
    "    #print(predictions)\n",
    "    #print(y_test)\n",
    "    \n",
    "    \n",
    "    rmse=np.sqrt(np.mean(((predictions[0]-y_test)**2)))\n",
    "    mape=np.mean(np.abs((predictions[0]-y_test)/y_test)*100)\n",
    "    #print(\"prediction:\", np.shape(pred))\n",
    "    #print(\"y_test:\",np.shape(y_test))\n",
    "    #print(len(continuous_ytest))\n",
    "    #print(\"continuouse_ytest: \", continuous_ytest)\n",
    "    #print(\"continuous_predictions: \", continuous_predictions)\n",
    "    return rmse, mape, continuous_ytest, continuous_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_correlation(arr1, arr2):\n",
    "    cov = np.cov(arr1, arr2)[0, 1]  # 공분산\n",
    "    arr1sd = np.std(arr1, ddof=1)   # x의 표본표준편차\n",
    "    arr2sd = np.std(arr2, ddof=1)   # y의 표본표준편차\n",
    "    return cov / ( arr1sd * arr2sd )\n",
    "    \n",
    "def get_time_gain(continuous_ytest, continuous_predictions):\n",
    "    corr = np.corrcoef(continuous_ytest[6:-6], continuous_predictions[6:-6])[0,1]\n",
    "    max_corr = corr\n",
    "    #j=1\n",
    "    max_trans = 0\n",
    "    for j in range(1, 6):\n",
    "        trans_corr = np.corrcoef(continuous_ytest[6:-6], continuous_predictions[6+j:-6+j])[0,1] \n",
    "        if max_corr < trans_corr:\n",
    "            max_corr = trans_corr\n",
    "            print(max_corr)\n",
    "            max_trans = j\n",
    "\n",
    "    return max_trans\n",
    "\n",
    "for i in range(15,21):\n",
    "    rmse, mape, continuous_ytest, continuous_predictions = test_model(model, i)\n",
    "    show_plots(i, continuous_ytest, continuous_predictions)\n",
    "    time_gain = get_time_gain(continuous_ytest, continuous_predictions)\n",
    "    print(\"Time_delay(min): \", time_gain*5)\n",
    "    print(\"Root-Mean-Squared Deviation {}\".format(rmse))\n",
    "    print(\"Mean-Absolute-Percentage-Error {}\".format(mape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('MLP_diabetes_model_1dropout_no_smoothing.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
