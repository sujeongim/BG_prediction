{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/79/r0nnwgwn76x8wcq77ydbjdx80000gn/T/ipykernel_71760/2591696125.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_measures\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrmse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMinMaxScaler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnp_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequence\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTimeseriesGenerator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from statsmodels.tools.eval_measures import rmse\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, TimeDistributed\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, Callback\n",
    "import warnings\n",
    "import io\n",
    "import math\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(a, n=10): \n",
    "    ret = np.cumsum(a, dtype=float)\n",
    "    ret[n:] = ret[n:] - ret[:-n]\n",
    "    ret/=n\n",
    "    # Masking\n",
    "    ret[:n-1]=ret[n-1]\n",
    "    ret=ret.reshape(-1,1)\n",
    "    return ret\n",
    "\n",
    "def read_train_patients(input_len = 8, output_len = 6):\n",
    "    x_train=[]\n",
    "    y_train=[]\n",
    "    #y_train_f=[]\n",
    "    for i in range(1, 15):\n",
    "        #y_noised=[]\n",
    "        \n",
    "        df=pd.read_csv(\"Virtual_patients_20\\V_CGMS_{}.csv\".format(i),header=None)\n",
    "        dataset=df.values\n",
    "        dataset = moving_average(dataset)\n",
    "\n",
    "        scaler = MinMaxScaler(feature_range=(0, 1)) \n",
    "        scaled_data = scaler.fit_transform(dataset)\n",
    "        train_data = scaled_data\n",
    "        #train_data = dataset\n",
    "        \n",
    "        for i in range(input_len,len(train_data)-output_len):\n",
    "            x_train.append(train_data[i-input_len:i,0])\n",
    "            y_train.append(train_data[i+output_len-1,0])\n",
    "        \n",
    "    x_train, y_train = np.array(x_train), np.array(y_train)\n",
    "    #reshape \n",
    "    #x_train = np.reshape(x_train, (x_train.shape[0],x_train.shape[1],1))\n",
    "    \n",
    "    return scaler, x_train, y_train\n",
    "    #return x_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(x_train, y_train, batch_size=1, epochs=1):\n",
    "    \n",
    "    X_val = x_train[11000:]\n",
    "    Y_val = y_train[11000:]\n",
    "    X_train = x_train[:11000]\n",
    "    Y_train = y_train[:11000]\n",
    "\n",
    "    \n",
    "    # MLP model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(32,input_dim=8,activation=\"relu\"))\n",
    "    #model.add(Dropout(0.3))\n",
    "    model.add(Dense(32,activation=\"relu\"))\n",
    "    #model.add(Dropout(0.3))\n",
    "    model.add(Dense(32, activation=\"relu\"))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(1))\n",
    "    print(model.summary())\n",
    "\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "    #over fitting 방지\n",
    "    early_stopping = EarlyStopping(monitor='loss', patience=50, mode='auto')\n",
    "    \n",
    "    #fit model\n",
    "    #hist = model.fit(X_train, Y_train, batch_size=batch_size, epochs=epochs, callbacks=[early_stopping],validation_data=(X_val, Y_val))\n",
    "    hist = model.fit(X_train, Y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_val, Y_val))\n",
    "    \n",
    "\n",
    "    # 학습과정 살펴보기\n",
    "    \"\"\"plt.plot(hist.history['loss'])\n",
    "    plt.plot(hist.history['val_loss'])\n",
    "    plt.ylim(0.0, 0.15)\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'val'], loc='upper left')\n",
    "    plt.show()\"\"\"\n",
    "    return model, hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler, x_train, y_train = read_train_patients()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, hist = train_model(x_train, y_train, batch_size=32, epochs=500) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_loss(hist):\n",
    "    plt.figure(figsize=(24,16))\n",
    "    plt.title('Loss after smoothing',fontsize=50)\n",
    "    plt.plot(hist.history['loss'], linewidth=5)\n",
    "    plt.plot(hist.history['val_loss'], linewidth=5)\n",
    "    plt.ylim(0, 0.0600)\n",
    "    ax=plt.axes()\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(0.005))\n",
    "    plt.xticks(fontsize=33)\n",
    "    plt.yticks(fontsize=33)\n",
    "    plt.ylabel('loss',fontsize=44)\n",
    "    plt.xlabel('epoch',fontsize=44)\n",
    "    plt.legend(['train', 'val'], loc='upper right', fontsize=44)\n",
    "    plt.show()\n",
    "\n",
    "plot_loss(hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#이건 바꾸지 말기 - 비교해야 되니깐\n",
    "def show_plots(i, continuous_ytest, continuous_predictions):\n",
    "    plt.figure(figsize=(24,16))\n",
    "    plt.title('Blood Glucose Prediction Model Result_patient_{}'.format(i),fontsize=40)\n",
    "    plt.plot(continuous_ytest, color = 'b', linewidth=3)\n",
    "    plt.plot(continuous_predictions, color = 'r', ls=\"--\", linewidth=3)\n",
    "    plt.xticks(fontsize=28)\n",
    "    plt.yticks(fontsize=28)\n",
    "    plt.ylim(40, 260)\n",
    "    ax=plt.axes()\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(20))\n",
    "    plt.xlabel('Timestamp',fontsize=28)\n",
    "    plt.ylabel('BG(CGM) (mg/dL)',fontsize=28)\n",
    "    plt.legend(['Real','Predictions'], loc='upper right',fontsize=32)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, i, input_len=8, output_len=6):\n",
    "    df = pd.read_csv(\"Virtual_patients_20\\V_CGMS_{}.csv\".format(i),header=None)\n",
    "    dataset=df.values\n",
    "    \n",
    "    dataset = moving_average(dataset)\n",
    "    \n",
    "    # Scalling data from 0 - 1 to input in the neural network\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1)) \n",
    "    scaled_data = scaler.fit_transform(dataset)\n",
    "    \n",
    "    x_test =[]\n",
    "    y_test =[]\n",
    "    \n",
    "    continuous_ytest=[]\n",
    "    i= input_len\n",
    "    \n",
    "    for i in range(input_len,len(dataset)-output_len):\n",
    "        x_test.append(scaled_data[i-input_len:i,0])\n",
    "        y_test.append(dataset[i+output_len-1,0])\n",
    "        continuous_ytest.append(dataset[i+output_len-1,0])\n",
    "\n",
    "    x_test = np.array(x_test)\n",
    "    y_test = np.array(y_test)\n",
    "    #y_test = np.reshape(y_test,(-1,1))\n",
    "    #print(\"x_test: \", x_test)\n",
    "    #x_test= np.reshape(x_test, (x_test.shape[0],x_test.shape[1],1))\n",
    "    \n",
    "    predictions = model.predict(x_test)\n",
    "    predictions = np.reshape(predictions,(1,-1))\n",
    "    predictions = scaler.inverse_transform(predictions)\n",
    "    print(predictions)\n",
    "    print(y_test)\n",
    "    #print(\"predictions: \", predictions)\n",
    "    \n",
    "    continuous_predictions = predictions[0]\n",
    "    for i in range(1, len(predictions)):\n",
    "        continuous_predictions = np.concatenate([continuous_predictions, predictions[i]])\n",
    "    \n",
    "    #print(predictions)\n",
    "    #print(y_test)\n",
    "    \n",
    "    \n",
    "    rmse=np.sqrt(np.mean(((predictions[0]-y_test)**2)))\n",
    "    mape=np.mean(np.abs((predictions[0]-y_test)/y_test)*100)\n",
    "    #print(\"prediction:\", np.shape(pred))\n",
    "    #print(\"y_test:\",np.shape(y_test))\n",
    "    #print(len(continuous_ytest))\n",
    "    #print(\"continuouse_ytest: \", continuous_ytest)\n",
    "    #print(\"continuous_predictions: \", continuous_predictions)\n",
    "    return rmse, mape, continuous_ytest, continuous_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_correlation(arr1, arr2):\n",
    "    cov = np.cov(arr1, arr2)[0, 1]  # 공분산\n",
    "    arr1sd = np.std(arr1, ddof=1)   # x의 표본표준편차\n",
    "    arr2sd = np.std(arr2, ddof=1)   # y의 표본표준편차\n",
    "    return cov / ( arr1sd * arr2sd )\n",
    "    \n",
    "def get_time_gain(continuous_ytest, continuous_predictions):\n",
    "    corr = np.corrcoef(continuous_ytest[6:-6], continuous_predictions[6:-6])[0,1]\n",
    "    max_corr = corr\n",
    "    #j=1\n",
    "    max_trans = 0\n",
    "    for j in range(1, 6):\n",
    "        trans_corr = np.corrcoef(continuous_ytest[6:-6], continuous_predictions[6+j:-6+j])[0,1] \n",
    "        if max_corr < trans_corr:\n",
    "            max_corr = trans_corr\n",
    "            print(max_corr)\n",
    "            max_trans = j\n",
    "\n",
    "    return max_trans\n",
    "\n",
    "for i in range(15,21):\n",
    "    rmse, mape, continuous_ytest, continuous_predictions = test_model(model, i)\n",
    "    show_plots(i, continuous_ytest, continuous_predictions)\n",
    "    time_gain = get_time_gain(continuous_ytest, continuous_predictions)\n",
    "    print(\"Time_delay(min): \", time_gain*5)\n",
    "    print(\"Root-Mean-Squared Deviation {}\".format(rmse))\n",
    "    print(\"Mean-Absolute-Percentage-Error {}\".format(mape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('MLP_diabetes_model_1dropout_no_smoothing.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
